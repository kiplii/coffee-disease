{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00c2a990-7e9e-4a04-ad37-92fe0d04b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "                               # IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f12b2e-d1e1-4f5f-b68f-ef13d1750834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from shutil import copyfile\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f65f320-404b-4ecc-85e2-dfa116024f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca75e224-c69c-4b97-babf-132e7c4319df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 3\n",
    "IMAGE_SIZE = 256 \n",
    "default_image_size = tuple((IMAGE_SIZE, IMAGE_SIZE)) \n",
    "image_size = 0 \n",
    "CHANNELS=3 \n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156d16ea-6e31-4dc7-a153-4b4e3a7860de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8db8e4-4f8f-40df-88e1-4bdcba1e522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09829db-f6e8-418d-9436-2167741e4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "                              # LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aaae12c-5ab2-4681-84e7-76be754cb9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19553 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PlantVillage\",\n",
    "    shuffle=True,\n",
    "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size= BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0585ff61-909a-4018-a141-1180366cd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "                           #  LISTING THE CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aab2441-a010-44d9-a5da-f7cfbd194eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cerscospora', 'Healthy', 'Leaf rust', 'Phoma']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87dde491-ecc2-4a65-aee5-ac26a19c14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "                          # Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "743f87a1-c93d-4d33-838d-6d2a9e67cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = ds.cardinality().numpy()\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d4b90a-5e20-415d-bfb6-03a89a8a4326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19553 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"PlantVillage\",\n",
    "  seed=123,\n",
    "  image_size=default_image_size,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e49158e-cdb3-4ba4-bbfc-e5d1940636c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bd719dd-0be2-4526-8661-1c2e1d509dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size=0.8\n",
    "len(dataset)*train_size\n",
    "train_ds=dataset.take(131)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2101f41d-62c3-411a-b4c2-ade83fbe0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8bb477c-055d-4e5b-8b62-39e6fe420608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds=dataset.skip(16)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56c84fb1-f22c-4c57-b755-42ad3e3fde0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Image preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc4ebf98-df48-43c6-8089-62f7cea7a405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds=test_ds.take(16)\n",
    "len(val_ds)\n",
    "test_ds=test_ds.skip(16)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190f5479-7c76-4ebe-b314-097dca544ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                         # Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e01a0ba7-beb4-4928-ab0e-790c040f269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    # Building Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5f73b7f-4f8f-4543-b58b-d961dd95feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "                          # Compiling and Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45a8c0e3-4d54-45ee-93b4-8d48a3627a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['Cerscospora', 'Healthy', 'Leaf rust', 'Phoma']\n"
     ]
    }
   ],
   "source": [
    "class_names = dataset.class_names\n",
    "n_classes = len(class_names)\n",
    "print(n_classes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3b674c3-cf2d-439c-804f-568480153693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da7bc7b1-1979-4660-a547-78dea848f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8eddd562-94d9-4242-afb0-a7dc00bfb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83f73f15-264f-4338-9732-2d9862b72a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "batch_input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    input_shape = (CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    batch_input_shape = (BATCH_SIZE, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    chanDim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "600ebde2-3774-4da5-a26f-98473a3d2a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=batch_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2307776c-bde7-42f3-8d18-84dd0b9dc007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (32, 254, 254, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (32, 127, 127, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (32, 125, 125, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (32, 62, 62, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (32, 60, 60, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (32, 30, 30, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (32, 28, 28, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (32, 14, 14, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (32, 12, 12, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (32, 6, 6, 64)            0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (32, 4, 4, 64)            36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (32, 2, 2, 64)            0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 256)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (32, 64)                  16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 4)                   260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183812 (718.02 KB)\n",
      "Trainable params: 183812 (718.02 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfd720f9-5d31-4c9a-baeb-06078cd46f82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48095205-afd3-45d2-9cb8-ad7893c3faa8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "131/131 [==============================] - 604s 4s/step - loss: 0.6122 - accuracy: 0.7696 - val_loss: 0.5177 - val_accuracy: 0.8008\n",
      "Epoch 2/15\n",
      "131/131 [==============================] - 589s 4s/step - loss: 0.3375 - accuracy: 0.8915 - val_loss: 0.2986 - val_accuracy: 0.8809\n",
      "Epoch 3/15\n",
      "131/131 [==============================] - 382s 3s/step - loss: 0.2612 - accuracy: 0.9105 - val_loss: 0.2803 - val_accuracy: 0.9316\n",
      "Epoch 4/15\n",
      "131/131 [==============================] - 385s 3s/step - loss: 0.2034 - accuracy: 0.9358 - val_loss: 0.2064 - val_accuracy: 0.9395\n",
      "Epoch 5/15\n",
      "131/131 [==============================] - 387s 3s/step - loss: 0.2055 - accuracy: 0.9356 - val_loss: 0.1890 - val_accuracy: 0.9453\n",
      "Epoch 6/15\n",
      "131/131 [==============================] - 363s 3s/step - loss: 0.1907 - accuracy: 0.9389 - val_loss: 0.2586 - val_accuracy: 0.9395\n",
      "Epoch 7/15\n",
      "131/131 [==============================] - 446s 3s/step - loss: 0.1506 - accuracy: 0.9556 - val_loss: 0.1204 - val_accuracy: 0.9629\n",
      "Epoch 8/15\n",
      "131/131 [==============================] - 490s 4s/step - loss: 0.1042 - accuracy: 0.9719 - val_loss: 0.0706 - val_accuracy: 0.9805\n",
      "Epoch 9/15\n",
      "131/131 [==============================] - 474s 4s/step - loss: 0.0902 - accuracy: 0.9773 - val_loss: 0.0577 - val_accuracy: 0.9902\n",
      "Epoch 10/15\n",
      "131/131 [==============================] - 502s 4s/step - loss: 0.0854 - accuracy: 0.9764 - val_loss: 0.0510 - val_accuracy: 0.9863\n",
      "Epoch 11/15\n",
      "131/131 [==============================] - 519s 4s/step - loss: 0.0655 - accuracy: 0.9833 - val_loss: 0.1826 - val_accuracy: 0.9707\n",
      "Epoch 12/15\n",
      "131/131 [==============================] - 499s 4s/step - loss: 0.0891 - accuracy: 0.9745 - val_loss: 0.1206 - val_accuracy: 0.9629\n",
      "Epoch 13/15\n",
      "131/131 [==============================] - 1138s 9s/step - loss: 0.0657 - accuracy: 0.9809 - val_loss: 0.1116 - val_accuracy: 0.9453\n",
      "Epoch 14/15\n",
      "131/131 [==============================] - 1127s 8s/step - loss: 0.0709 - accuracy: 0.9795 - val_loss: 0.0202 - val_accuracy: 0.9941\n",
      "Epoch 15/15\n",
      "131/131 [==============================] - 609s 5s/step - loss: 0.0594 - accuracy: 0.9852 - val_loss: 0.0260 - val_accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=15,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2e9ace7-dadf-4831-b8ad-38779710b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                # Saving Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "754a47ab-f3a3-46b5-bd18-948fc4d6d30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef707fcf-ebe6-4d21-ac2f-7b7df476699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 160s 1s/step - loss: 0.0238 - accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "#Training set Accuracy\n",
    "scores = model.evaluate(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f78837c1-9a53-4b83-96e2-bfcdc8c12227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 28s 1s/step - loss: 0.0260 - accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "#Validation set Accuracy\n",
    "scores = model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "634aa58e-5922-4e2e-95ed-13d322e207cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 #Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d2ef3be-aa9c-4250-ab93-342f0cda517f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/16\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/16\\assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_version = max([int(i) for i in (os.listdir(f\"./Model\")+[0])]) + 1\n",
    "model.save(f\"./Model/{model_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6128450f-dde8-4b72-8e2e-5a45e0f78073",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 #Saving H5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e878494-fb4e-4858-aefe-6d8b1c4dda9f",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\820 G3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('coffee_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "438e95a6-2e61-4997-bc49-6c5691e80e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 #Saving Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9aa10cc-9557-4be3-bf5d-1a2cd994be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('disease.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff81c829-c36e-48d9-8f54-2e0bc3fa6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "                           # Converting Model to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85d11835-d690-4901-9506-62eba2de361f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\820G3~1\\AppData\\Local\\Temp\\tmpql2xtcmv\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\820G3~1\\AppData\\Local\\Temp\\tmpql2xtcmv\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82f914de-e9ee-4834-bef6-7e6d7edda678",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coffee.tflite\",'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05a35495-7aef-4a31-b7ef-440c08efe000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6122015118598938,\n",
       "  0.3374897539615631,\n",
       "  0.261168897151947,\n",
       "  0.2033757120370865,\n",
       "  0.20547881722450256,\n",
       "  0.19065330922603607,\n",
       "  0.1505882889032364,\n",
       "  0.10420755296945572,\n",
       "  0.0902390405535698,\n",
       "  0.08543858677148819,\n",
       "  0.06547963619232178,\n",
       "  0.08911720663309097,\n",
       "  0.06566452980041504,\n",
       "  0.07085878401994705,\n",
       "  0.05939154699444771],\n",
       " 'accuracy': [0.7695610523223877,\n",
       "  0.8914599418640137,\n",
       "  0.9105439186096191,\n",
       "  0.9358301758766174,\n",
       "  0.9355915784835815,\n",
       "  0.9389312863349915,\n",
       "  0.9556297659873962,\n",
       "  0.9718511700630188,\n",
       "  0.9773377776145935,\n",
       "  0.9763835668563843,\n",
       "  0.9833015203475952,\n",
       "  0.9744752049446106,\n",
       "  0.9809160232543945,\n",
       "  0.9794847369194031,\n",
       "  0.9852099418640137],\n",
       " 'val_loss': [0.5177059769630432,\n",
       "  0.29857128858566284,\n",
       "  0.280348002910614,\n",
       "  0.20640863478183746,\n",
       "  0.18903027474880219,\n",
       "  0.25857609510421753,\n",
       "  0.12040208280086517,\n",
       "  0.07058554887771606,\n",
       "  0.05768671631813049,\n",
       "  0.05103199556469917,\n",
       "  0.1825566291809082,\n",
       "  0.1205848976969719,\n",
       "  0.11156624555587769,\n",
       "  0.020162003114819527,\n",
       "  0.026040900498628616],\n",
       " 'val_accuracy': [0.80078125,\n",
       "  0.880859375,\n",
       "  0.931640625,\n",
       "  0.939453125,\n",
       "  0.9453125,\n",
       "  0.939453125,\n",
       "  0.962890625,\n",
       "  0.98046875,\n",
       "  0.990234375,\n",
       "  0.986328125,\n",
       "  0.970703125,\n",
       "  0.962890625,\n",
       "  0.9453125,\n",
       "  0.994140625,\n",
       "  0.9921875]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history #Return Dictionary of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aedc7f2-0c7f-460d-b8e5-67e1de1e4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording History in json\n",
    "import json\n",
    "with open('training_hist.json','w') as f:\n",
    "  json.dump(history.history,f)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0532eae-a7e1-42bd-ae7f-50d187621b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "                          #Calculating Accuracy of Model Achieved on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad35f549-a23e-4305-a127-bbcd9e1929e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set Accuracy: 99.21875 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation set Accuracy: {} %\".format(history.history['val_accuracy'][-1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "828d9be4-b3bb-4fd0-a671-459cb2046aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "                    #Test set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9733fe03-81f2-4960-92b0-7ed3e24e20f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19553 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PlantVillage\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e84b194b-b49b-4c84-981d-1becfbe39c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "407b40fc-4b41-496a-a0d1-7604caf88ce3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3055\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3053\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3055\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3057\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5886\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5887\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5888\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]] [Op:IteratorGetNext] name: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        \n",
    "        actual_class = class_names[labels[i]] \n",
    "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3b431-b9f5-4795-a83b-a8637195d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                               #Accuracy Visualization \n",
    "\n",
    "                                               #Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c3840-0a05-4769-8b61-2206dbc4ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(0,15)]\n",
    "plt.plot(epochs,history.history['accuracy'],color='red')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Visualization of Training Accuracy Result')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e53b0c-5c1c-41b0-ba6d-7f91c6d36a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 #Validation Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652be068-fe67-47f7-98ac-cddae9762c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, history.history['val_accuracy'],color='blue')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Visualization of Validation Accuracy Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62213469-525c-4d62-9b33-3661058a455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                 #Training and Validation Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27d695-2a4e-4392-9b35-e0aa91916635",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237ba5d-d49c-4784-b567-6db68ddad231",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "epochs = [i for i in range(0,15)]\n",
    "plt.plot(epochs, acc, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b52cd-a931-4aeb-9a08-ce9eb310973a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed45c5-36e6-4ccd-8bae-5c01da6f5de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dc55c-0abc-4e7b-a792-190ef0f1c55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260d4e2-7888-4c4b-81e3-78ef26410f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b5d0d-2792-462c-8933-58e74aa55700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdb126-35e0-4572-a4ee-ff0a0440ef68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6d8cd-5ead-4a04-b88c-2d5fd87b9abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5ce147-a33a-43ac-8f18-31abb6690807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
